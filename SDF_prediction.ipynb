{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDFDataset(Dataset):\n",
    "    \"\"\"Dataset for loading SDF csv file.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_filename, root_dir='.'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): path to the csv file.\n",
    "            root_dir (string): directory with data.\"\"\"\n",
    "        \n",
    "        self.full_path = os.path.join(root_dir, csv_filename)\n",
    "        self.df = pd.read_csv(self.full_path).to_numpy()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.df[idx]\n",
    "    \n",
    "    \n",
    "class SDFDatasetFromDF(Dataset):\n",
    "    \"\"\"Dataset for loading SDF dataframe.\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): pandas dataframe with data.\"\"\"\n",
    "        \n",
    "        self.df = df.to_numpy()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06561273,  0.3669782 , -0.39920893,  0.00078972],\n",
       "       [ 0.34411201,  0.38627106,  0.17548348, -0.00232207],\n",
       "       [-0.08663495,  0.30739027, -0.38965395,  0.00155663],\n",
       "       [-0.20267186,  0.45919091, -0.17262226,  0.00096568],\n",
       "       [-0.05268847,  0.22936745,  0.48423633, -0.00202632]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking SDFDataset\n",
    "folder = '02818832'\n",
    "model_dir = '10c15151ebe3d237240ea0cdca7b391a'\n",
    "root_dir = os.path.join('data', folder, model_dir, 'models')\n",
    "\n",
    "dataset = SDFDataset('sdf.csv', root_dir=root_dir)\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([32, 4])\n",
      "1 torch.Size([32, 4])\n",
      "2 torch.Size([32, 4])\n",
      "3 torch.Size([32, 4])\n",
      "4 torch.Size([32, 4])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched.size())\n",
    "    if i_batch == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>sdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.073887</td>\n",
       "      <td>-0.033628</td>\n",
       "      <td>-0.102642</td>\n",
       "      <td>0.013775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.234475</td>\n",
       "      <td>0.353427</td>\n",
       "      <td>0.491546</td>\n",
       "      <td>0.068524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.983944</td>\n",
       "      <td>-0.988168</td>\n",
       "      <td>-0.995850</td>\n",
       "      <td>-0.109368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.144064</td>\n",
       "      <td>-0.358193</td>\n",
       "      <td>-0.567773</td>\n",
       "      <td>-0.000533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.086747</td>\n",
       "      <td>-0.111310</td>\n",
       "      <td>-0.131580</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.311107</td>\n",
       "      <td>0.324987</td>\n",
       "      <td>0.353356</td>\n",
       "      <td>0.000913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.991839</td>\n",
       "      <td>0.990133</td>\n",
       "      <td>0.980318</td>\n",
       "      <td>0.792862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   x              y              z            sdf\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000\n",
       "mean        0.073887      -0.033628      -0.102642       0.013775\n",
       "std         0.234475       0.353427       0.491546       0.068524\n",
       "min        -0.983944      -0.988168      -0.995850      -0.109368\n",
       "25%        -0.144064      -0.358193      -0.567773      -0.000533\n",
       "50%         0.086747      -0.111310      -0.131580       0.000235\n",
       "75%         0.311107       0.324987       0.353356       0.000913\n",
       "max         0.991839       0.990133       0.980318       0.792862"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data in .csv format\n",
    "df = pd.read_csv(os.path.join(root_dir, 'sdf.csv'))\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.46513540e-01,  3.56236666e-01, -1.80856764e-01,\n",
       "         6.10453426e-04],\n",
       "       [ 1.17236823e-01,  4.04113799e-01, -5.70215046e-01,\n",
       "        -1.93676760e-03],\n",
       "       [ 1.04545362e-01, -2.07269251e-01, -7.77650714e-01,\n",
       "        -8.69859825e-04],\n",
       "       [ 1.43708557e-01,  4.06823397e-01, -5.73988914e-01,\n",
       "        -1.20211858e-03],\n",
       "       [ 1.09590739e-01, -2.20363244e-01,  7.64812052e-01,\n",
       "         4.87884885e-04]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking SDFDatasetFromDF\n",
    "\n",
    "# Splitting into training and testing data.\n",
    "train_data, test_data = train_test_split(df)\n",
    "\n",
    "train_dataset = SDFDatasetFromDF(train_data)\n",
    "test_dataset = SDFDatasetFromDF(test_data)\n",
    "\n",
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([32, 4])\n",
      "1 torch.Size([32, 4])\n",
      "2 torch.Size([32, 4])\n",
      "3 torch.Size([32, 4])\n",
      "4 torch.Size([32, 4])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32,\n",
    "                              shuffle=True, num_workers=4)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "    print(i_batch, sample_batched.size())\n",
    "    if i_batch == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDeepSDF(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepSDF for one 3d shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleDeepSDF, self).__init__()\n",
    "        \n",
    "        # input of 3 - (x,y,z)\n",
    "        self.fc1 = nn.Linear(3, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 509)\n",
    "        # between this layers we use input again\n",
    "        self.fc5 = nn.Linear(512, 512)\n",
    "        self.fc6 = nn.Linear(512, 512)\n",
    "        self.fc7 = nn.Linear(512, 512)\n",
    "        self.fc8 = nn.Linear(512, 1)\n",
    "        self.tanh = nn.Tanh()  # activation layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        inpt = x.detach().clone().type(torch.FloatTensor)  # input vector\n",
    "\n",
    "        x = F.relu(self.fc1(inpt))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        # using input again\n",
    "        x = torch.cat((x, inpt), dim=1)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "\n",
    "        x = torch.tanh(x)  # activation\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSDF(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepSDF for multiple 3d shapes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DeepSDF, self).__init__()\n",
    "        \n",
    "        # Latent vector needs to be reinitialized\n",
    "        self.lat_vec = np.random.sample(256)\n",
    "        \n",
    "        # input of 259 -> 256 - latent vector, 3 - (x,y,z)\n",
    "        self.fc1 = nn.Linear(259, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 253)\n",
    "        # between this layers we use input again\n",
    "        self.fc5 = nn.Linear(512, 512)\n",
    "        self.fc6 = nn.Linear(512, 512)\n",
    "        self.fc7 = nn.Linear(512, 512)\n",
    "        self.fc8 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        inpt = np.concatenate(self.lat_vec, x)  # input vector\n",
    "\n",
    "        x = F.relu(self.fc1(inpt))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        # using input again\n",
    "        x = np.concatenate(x, inpt)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.relu(self.fc8(x))\n",
    "        x = self.tanh(x)  # activation\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # TODO\n",
    "    def load_latent_vector(filename):\n",
    "        pass\n",
    "    \n",
    "    # TODO\n",
    "    def save_latent_vector(filename):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleDeepSDF(\n",
      "  (fc1): Linear(in_features=3, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc4): Linear(in_features=512, out_features=509, bias=True)\n",
      "  (fc5): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc6): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc7): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc8): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SimpleDeepSDF()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_loss(output, target, dlt=0.5):\n",
    "    return torch.sum(torch.abs(torch.clamp(output, -dlt, dlt) - torch.clamp(target, -dlt, dlt).view(-1, 1)))\n",
    "    \n",
    "criterion = sdf_loss  # Loss\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 0.370\n",
      "[1,  2000] loss: 0.309\n",
      "[2,  1000] loss: 0.304\n",
      "[2,  2000] loss: 0.270\n",
      "[3,  1000] loss: 0.257\n",
      "[3,  2000] loss: 0.222\n",
      "[4,  1000] loss: 0.229\n",
      "[4,  2000] loss: 0.203\n",
      "[5,  1000] loss: 0.130\n",
      "[5,  2000] loss: 0.118\n",
      "[6,  1000] loss: 0.114\n",
      "[6,  2000] loss: 0.118\n",
      "[7,  1000] loss: 0.109\n",
      "[7,  2000] loss: 0.114\n",
      "[8,  1000] loss: 0.109\n",
      "[8,  2000] loss: 0.110\n",
      "[9,  1000] loss: 0.105\n",
      "[9,  2000] loss: 0.104\n",
      "[10,  1000] loss: 0.103\n",
      "[10,  2000] loss: 0.101\n",
      "[11,  1000] loss: 0.106\n",
      "[11,  2000] loss: 0.098\n",
      "[12,  1000] loss: 0.100\n",
      "[12,  2000] loss: 0.103\n",
      "[13,  1000] loss: 0.102\n",
      "[13,  2000] loss: 0.101\n",
      "[14,  1000] loss: 0.101\n",
      "[14,  2000] loss: 0.095\n",
      "[15,  1000] loss: 0.094\n",
      "[15,  2000] loss: 0.100\n",
      "[16,  1000] loss: 0.094\n",
      "[16,  2000] loss: 0.096\n",
      "[17,  1000] loss: 0.097\n",
      "[17,  2000] loss: 0.091\n",
      "[18,  1000] loss: 0.090\n",
      "[18,  2000] loss: 0.097\n",
      "[19,  1000] loss: 0.095\n",
      "[19,  2000] loss: 0.091\n",
      "[20,  1000] loss: 0.089\n",
      "[20,  2000] loss: 0.092\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[:, :3], data[:, 3]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type SimpleDeepSDF. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net, os.path.join('models', 'third_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model + Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleDeepSDF(\n",
       "  (fc1): Linear(in_features=3, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=509, bias=True)\n",
       "  (fc5): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc6): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc7): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc8): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.load(os.path.join('models', 'third_model'))\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of all test data: 0.002843\n",
      "\"result.csv\" is generated.\n"
     ]
    }
   ],
   "source": [
    "# Testing and saving results for graphic representation\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "text = 'x,y,z,sdf\\n'\n",
    "loss = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        input, labels = data[:, :3], data[:, 3]\n",
    "        \n",
    "        outputs = net(input)\n",
    "        text += '{},{},{},{}\\n'.format(input[0][0], input[0][1], input[0][2], outputs[0][0])\n",
    "        loss += (np.abs(outputs - labels)).sum()\n",
    "\n",
    "print('Average loss of all test data: %f' % (loss / len(test_dataset)))\n",
    "\n",
    "with open(os.path.join('data', folder, model_dir, 'models', 'result.csv'), 'w') as f:\n",
    "    f.write(text)\n",
    "    \n",
    "print('\"result.csv\" is generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model outputs [0.0005956334643997252], while the true value is [-0.0019950037822127342].\n"
     ]
    }
   ],
   "source": [
    "output = net(data[:, :3])\n",
    "print(\"Model outputs [{}], while the true value is [{}].\".format(output[0, 0], data[0, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
